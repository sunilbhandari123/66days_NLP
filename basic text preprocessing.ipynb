{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3080b510",
   "metadata": {},
   "source": [
    "# Basic Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b901f800",
   "metadata": {},
   "source": [
    "# Removing html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcf575ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b324e2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "999c79a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\"A wonderful little production. <br /><br />The filming\n",
    "technique is very unassuming- very old-time-B...\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b72c321",
   "metadata": {},
   "outputs": [],
   "source": [
    " # create a function that takes the pattern and replace with blankspace\n",
    "def remove_urls(text):\n",
    "    url_patterns=r\"<.*?>\"\n",
    "    return re.sub(url_patterns,\"\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11c7ba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function that uses a above function and adds it to  our pipline\n",
    "def process(text):\n",
    "    clean_text=remove_urls(text)\n",
    "    doc=nlp(clean_text)\n",
    "    return(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "400230d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=process(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aebf285d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "A\n",
      "wonderful\n",
      "little\n",
      "production\n",
      ".\n",
      "The\n",
      "filming\n",
      "\n",
      "\n",
      "technique\n",
      "is\n",
      "very\n",
      "unassuming-\n",
      "very\n",
      "old\n",
      "-\n",
      "time\n",
      "-\n",
      "B\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "for ent in doc:\n",
    "    print(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c4e76aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here all the htmls text has been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633402be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b131df23",
   "metadata": {},
   "source": [
    "# Removing Punctuations and lowercasing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5565c573",
   "metadata": {},
   "source": [
    "why we need to remove punctuations?\n",
    "If there is puncution in the text then there is two possibilities\n",
    "\n",
    "1. when we will do tokonization then all puncutation marks will be treated as word therefore our model will be confused\n",
    "\n",
    "2. It can treat word and puncutaion as a single word (eg hello and hello!) are treated two different words which might cause problem while treating a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94c8444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1=\"Hello said sunil and introduced ,Hello! i am manish currently working in NLP i have self-learning.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e9edb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1=nlp(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "655be62d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello said sunil and introduced hello i am manish currently working in nlp i have self learning'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_no_pun=[token.text for token in doc1 if not token.is_punct]\n",
    "' '.join(text_no_pun).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a65eca",
   "metadata": {},
   "source": [
    "# Chat word treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b56129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words ={\n",
    "\"U\":\"you\",\n",
    "'AFAIK' : 'As Far As I Know',\n",
    "\"AFK\":\"Away From Keyboard\",\n",
    "\"ASAP\":\"As Soon As Possible\",\n",
    "\"ATK\":\"At The Keyboard\",\n",
    "\"ATM\":\"At The Moment\",\n",
    "\"A3\":\"Anytime, Anywhere, Anyplace\",\n",
    "\"BAK\":\"Back At Keyboard\",\n",
    "\"BB\":\"Be Back Later\",\n",
    "\"BBS\":\"Be Back Soon\",\n",
    "\"BFN\":\"Bye For Now\",\n",
    "\"B4N\":\"Bye For Now\",\n",
    "\"BRB\":\"Be Right Back\",\n",
    "\"BRT\":\"Be Right There\",\n",
    "\"BTW\":\"By The Way\",\n",
    "\"B4\":\"Before\",\n",
    "\"B4N\":\"Bye For Now\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f9271c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdf40990",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"word_treatment\")\n",
    "def word_treatment(textt):\n",
    "    new_text=[]\n",
    "    words=textt.split()\n",
    "    full_words=[chat_words.get(word.upper(),word)for word in words]\n",
    "    return ' '.join(full_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7db59a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chat(textt):\n",
    "    full_word=word_treatment(textt)\n",
    "    doc3=nlp(full_word)\n",
    "    return doc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8671812d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you\n",
      "go\n",
      "no\n",
      "Bye\n",
      "For\n",
      "Now\n"
     ]
    }
   ],
   "source": [
    "chat_text=\"u go no bfn\"\n",
    "doc3=process_chat(chat_text)\n",
    "for ent in doc3:\n",
    "    print(ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39e6595",
   "metadata": {},
   "source": [
    "# Spelling Correction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0440f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install contextualSpellCheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01035c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abd0984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8a8d4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07114f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Income was $9.4 million compared to the prior year of $2.7 million.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import contextualSpellCheck\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "contextualSpellCheck.add_to_pipe(nlp)\n",
    "doc = nlp('Income was $9.4 milion compared to the prioer year of $2.7 milion.')\n",
    "\n",
    "print(doc._.performed_spellCheck) #Should be True\n",
    "print(doc._.outcome_spellCheck)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa553cf8",
   "metadata": {},
   "source": [
    "# Removing Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68d0d9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text='''hi i am sunil bhandari passionate to learn about nlp where i am trying to be consistent and learn \n",
    "the new concept in the field of datascinece and nlp'''\n",
    "doc=nlp(text)\n",
    "token=[token for token in doc if not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c801831e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[hi,\n",
       " sunil,\n",
       " bhandari,\n",
       " passionate,\n",
       " learn,\n",
       " nlp,\n",
       " trying,\n",
       " consistent,\n",
       " learn,\n",
       " ,\n",
       " new,\n",
       " concept,\n",
       " field,\n",
       " datascinece,\n",
       " nlp]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecf82b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
