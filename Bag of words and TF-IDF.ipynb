{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aba21ef6",
   "metadata": {},
   "source": [
    "# Bag of words and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fef816f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f3bf4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f61d56c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('Toxic comment data kaggle/train.csv/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a73b289d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7325ecfa",
   "metadata": {},
   "source": [
    "#lets take only comments_text and toxic for now and implement our bags of word and TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ebd36e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(columns=['id','severe_toxic','obscene','threat','insult','identity_hate'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0414cc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= data[:10000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5fa289d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85ba2304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "61a154e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'whereby', 'please', 'whence', 'neither', 'while', 'those', 'much', 'through', 'her', 'when', 'wherein', 'move', 'yourself', 'for', 'regarding', 'latterly', 'that', 'although', 'twelve', 'which', 'show', 'the', 'bottom', 'in', 'none', 'among', 'they', 'call', 'doing', 'seem', 'hundred', 'due', 'former', 'was', \"'ll\", 'onto', 'how', 'whereafter', 'every', 'nor', 'he', 'give', '‘ll', '’s', 'became', 'across', 'just', 'therein', 'after', 'many', 'between', 'always', 'several', 'where', 'next', '’ve', 'together', 'anyone', 'as', 'whose', 'an', 'three', 'into', 'were', 'full', 'alone', 'but', 'anything', 'from', 'with', 'mostly', 'my', 'down', 'behind', 'these', 'over', \"'ve\", 'side', 'noone', 'forty', 'formerly', 'by', 'seeming', 'often', 'more', '‘d', 'everywhere', 'i', 'n’t', 'else', 'rather', 'so', 'a', 'already', 'via', 'less', 'thereafter', 'whenever', 'back', 'however', 'had', 'is', 'own', 'itself', 'about', 'two', 'your', 'seems', 'take', 'thru', 'toward', 'above', 'nevertheless', 'elsewhere', 'then', 'yet', 'been', 'unless', 'why', 'anyhow', 'can', 'she', 'wherever', 'someone', 'on', 'various', 'again', 'thereupon', 'thus', 'sometimes', '’d', '‘s', 'must', 'per', 'others', 'here', 'towards', '‘ve', 'such', 'who', 'enough', 'whom', 'along', 'hereafter', 'we', 'since', 'of', 'get', 'herein', 'within', 'ours', \"'m\", 'meanwhile', 'becoming', 'will', 'below', 'no', 'first', 'not', 'am', 'you', 'few', 'say', 'amongst', 'herself', 'whole', 'may', 'yourselves', 'ten', 'also', 'empty', 'either', '’m', 'somewhere', 'hereby', 'whether', 're', 'too', 'might', 'each', 'yours', 'because', 'whereupon', 'n‘t', 'them', 'ourselves', 'hence', 'beforehand', 'fifteen', 'least', 'until', 'serious', 'without', 'me', 'could', 'becomes', 'top', 'nothing', 'our', 'front', 'throughout', 'used', 'beside', 'eleven', '‘re', 'once', 'most', 'this', 'nowhere', 'under', 'their', 'never', 'thence', 'be', 'something', 'than', 'to', 'off', 'at', 'hers', 'out', 'upon', 'have', 'name', 'hereupon', 'now', 'did', 'being', 'four', 'his', 'and', 'anywhere', '‘m', \"n't\", 'indeed', 'third', 'thereby', 'afterwards', 'eight', 'perhaps', 'really', 'part', 'ca', 'done', 'everyone', 'twenty', 'further', 'if', 'using', 'us', \"'d\", 'five', \"'re\", 'themselves', 'it', 'only', 'quite', 'nine', 'what', 'six', 'keep', 'one', 'except', 'another', \"'s\", 'other', 'see', 'should', 'does', 'still', 'whereas', 'therefore', 'its', 'around', 'whither', 'fifty', '’re', 'even', 'all', 'seemed', 'do', 'sometime', 'there', 'though', 'go', 'same', 'himself', 'everything', 'has', 'namely', 'latter', 'cannot', 'besides', '’ll', 'mine', 'last', 'sixty', 'well', 'both', 'would', 'myself', 'otherwise', 'become', 'before', 'up', 'very', 'almost', 'whoever', 'whatever', 'nobody', 'put', 'somehow', 'some', 'beyond', 'against', 'or', 'make', 'him', 'any', 'amount', 'anyway', 'moreover', 'are', 'during', 'ever', 'made'}\n"
     ]
    }
   ],
   "source": [
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "stop_words=nlp.Defaults.stop_words\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ad4cb7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "Punctuations=string.punctuation\n",
    "print(Punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "43241dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function that will capture the exclude the stop word and punctutaions from our data\n",
    "def spacy_fun(sentence):\n",
    "    doc=nlp(sentence)#creating our token object which is used to create documents with linguistic component\n",
    "    mytokens=[word.lemma_.lower().strip() for word in doc]# lemitazation each word and converting each token into lowercase\n",
    "    mytokens=[word for word in mytokens if word not in stop_words and word not in Punctuations]# Removing stops words and punctuations\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c616fc54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learn', 'natural', 'language', 'processing']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check our fucnction is working fine or not in a simple model to get understanding\n",
    "sentence=\"I am learning Natural language processing\"\n",
    "spacy_fun(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a42c48d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It seems like it is working fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "13953297",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vector=CountVectorizer(tokenizer=spacy_fun)# passing our function into countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7d6653b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets cee how countVectorizer works in a simple text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5ee75ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 1],\n",
       "       [1, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vector.fit_transform([\" I am learning natural language processing\" ,\"I am enjoying it\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "10850bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['enjoy', 'language', 'learn', 'natural', 'processing'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vector.get_feature_names_out() # gives the list of feature of out text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "40567c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learn': 2, 'natural': 3, 'language': 1, 'processing': 4, 'enjoy': 0}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vector.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b169f9",
   "metadata": {},
   "source": [
    "# Machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e8144dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X=data['comment_text']\n",
    "y=data['toxic']\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7da8368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b4b4a29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6625b706",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectors=count_vector.fit_transform(X_train)\n",
    "X_test_vectors=count_vector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "df4f4346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 29351)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectors.shape\n",
    "X_test_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fce2e0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectors.toarray()\n",
    "X_test_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5b1d6bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train_vectors,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "db70d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted=classifier.predict(X_test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "414cb450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9445\n",
      "Precision 0.816793893129771\n",
      "Recall 0.5515463917525774\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Precision\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Recall\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff89e9aa",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "249e9130",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector=TfidfVectorizer(tokenizer=spacy_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "628146ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectors=tfidf_vector.fit_transform(X_train)\n",
    "X_test_vectors=tfidf_vector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cee1dead",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d4a77783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train_vectors,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c83d6a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted=classifier.predict(X_test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "25f1fdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.936\n",
      "Precision 0.9852941176470589\n",
      "Recall 0.34536082474226804\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Precision\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Recall\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f41124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
